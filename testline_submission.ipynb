{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb2l-NbQI1jB"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Fetch Current Quiz Data\n",
        "current_quiz_data_url = \"https://www.jsonkeeper.com/b/LLQT\"\n",
        "current_quiz_data = requests.get(current_quiz_data_url).json()\n",
        "\n",
        "# Fetch Historical Quiz Data\n",
        "historical_quiz_data_url = \"https://api.jsonserve.com/XgAgFJ\"\n",
        "historical_quiz_data = requests.get(historical_quiz_data_url).json()\n",
        "\n",
        "# Print the first few records to check the data\n",
        "print(\"Current Quiz Data Sample:\")\n",
        "print(current_quiz_data)\n",
        "\n",
        "print(\"Historical Quiz Data Sample:\")\n",
        "print(historical_quiz_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the structure of the current quiz data\n",
        "print(\"Keys in Current Quiz Data:\")\n",
        "print(current_quiz_data.keys())\n",
        "\n",
        "# Print a sample of the quiz to understand its structure\n",
        "print(\"Sample of Current Quiz Data:\")\n",
        "print(current_quiz_data)"
      ],
      "metadata": {
        "id": "VKsQcxaYI4Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first few items of the Historical Quiz Data\n",
        "print(\"First few entries in Historical Quiz Data:\")\n",
        "print(historical_quiz_data[:3])  # Print the first 3 records for inspection"
      ],
      "metadata": {
        "id": "tPTDiz5KLydN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in each record of the historical quiz data\n",
        "historical_quiz_data_missing = []\n",
        "for record in historical_quiz_data:\n",
        "    missing_fields = [key for key, value in record.items() if not value]\n",
        "    if missing_fields:\n",
        "        historical_quiz_data_missing.append({record['id']: missing_fields})\n",
        "\n",
        "# Print missing fields for each quiz record\n",
        "print(\"Missing fields in Historical Quiz Data:\")\n",
        "print(historical_quiz_data_missing)"
      ],
      "metadata": {
        "id": "nB8C5inbNGvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the current quiz data\n",
        "current_quiz_data_missing = [key for key, value in current_quiz_data.items() if not value]\n",
        "print(\"Missing fields in Current Quiz Data:\", current_quiz_data_missing)"
      ],
      "metadata": {
        "id": "Oql0sA3xMLaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert historical quiz data (list of dictionaries) into a DataFrame\n",
        "historical_df = pd.DataFrame(historical_quiz_data)\n",
        "\n",
        "# Check the structure of the DataFrame to ensure everything is loaded correctly\n",
        "print(historical_df.head())"
      ],
      "metadata": {
        "id": "qEDQkly2Nu1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PUbJubjD_uUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import json_normalize\n",
        "\n",
        "# Normalize the Current Quiz Data dictionary\n",
        "current_quiz_df = json_normalize(current_quiz_data)\n",
        "\n",
        "# Check the structure of the Current Quiz DataFrame\n",
        "print(\"Current Quiz DataFrame:\")\n",
        "print(current_quiz_df)"
      ],
      "metadata": {
        "id": "8IJSLKH2NVTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by 'quiz' to analyze topic-wise performance in Historical Data\n",
        "# Assuming 'quiz' column actually contains a dictionary with a key like 'topic'\n",
        "# that represents the quiz topic\n",
        "topic_performance_historical = historical_df.groupby(historical_df['quiz'].apply(lambda x: x['topic'])).agg(\n",
        "    avg_accuracy=('accuracy', lambda x: pd.to_numeric(x.str.rstrip(' %')).mean()),  # Convert accuracy to numeric before calculating mean\n",
        "    total_correct_answers=('correct_answers', 'sum'),\n",
        "    total_incorrect_answers=('incorrect_answers', 'sum'),\n",
        "    total_score=('score', 'mean')\n",
        ").reset_index()\n",
        "\n",
        "print(\"Topic Performance (Historical):\")\n",
        "print(topic_performance_historical)"
      ],
      "metadata": {
        "id": "qviKJ8pa_vy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract relevant fields from the Current Quiz Data\n",
        "current_topic = current_quiz_df['quiz.topic'].iloc[0]  # Accessing nested 'topic'\n",
        "questions = current_quiz_df['quiz.questions'].iloc[0]\n",
        "\n",
        "# Example: Calculate total questions and answers (assuming responses are available)\n",
        "total_questions = len(questions)\n",
        "correct_answers = sum([1 for q in questions if q.get('is_correct')])\n",
        "accuracy = correct_answers / total_questions if total_questions > 0 else 0\n",
        "\n",
        "print(f\"Current Quiz - Topic: {current_topic}, Accuracy: {accuracy:.2%}, Correct Answers: {correct_answers}/{total_questions}\")"
      ],
      "metadata": {
        "id": "r6LUc4bL_1bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Analyze performance by difficulty level in Current Quiz Data\n",
        "difficulty_levels = [q.get('difficulty') for q in questions if q.get('difficulty')]\n",
        "\n",
        "# Count questions by difficulty (if available)\n",
        "difficulty_counts = pd.Series(difficulty_levels).value_counts()\n",
        "print(\"Current Quiz - Question Count by Difficulty:\")\n",
        "print(difficulty_counts)"
      ],
      "metadata": {
        "id": "pxfplK7LA54a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'initial_mistake_count' and 'better_than' columns exist in historical_df\n",
        "\n",
        "# Calculate a difficulty score\n",
        "historical_df['difficulty_score'] = (historical_df['initial_mistake_count'] / historical_df['better_than'])\n",
        "\n",
        "# Define difficulty levels based on score ranges\n",
        "def assign_difficulty(score):\n",
        "    if score <= 0.2:\n",
        "        return 'Easy'\n",
        "    elif score <= 0.5:\n",
        "        return 'Medium'\n",
        "    else:\n",
        "        return 'Hard'\n",
        "\n",
        "historical_df['difficulty_level'] = historical_df['difficulty_score'].apply(assign_difficulty)\n",
        "\n",
        "# Group by difficulty level and calculate performance metrics\n",
        "# Assuming 'accuracy' column contains the accuracy values as strings with '%'\n",
        "historical_df['calculated_accuracy'] = pd.to_numeric(historical_df['accuracy'].str.rstrip('%')) / 100  # Convert accuracy to numeric\n",
        "difficulty_performance = historical_df.groupby('difficulty_level').agg(\n",
        "    avg_accuracy=('calculated_accuracy', 'mean'),\n",
        "    total_correct_answers=('correct_answers', 'sum'),\n",
        "    total_score=('score', 'mean')\n",
        ").reset_index()\n",
        "\n",
        "print(\"Difficulty-Level Performance:\")\n",
        "print(difficulty_performance)"
      ],
      "metadata": {
        "id": "A5yVTGjLBnbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate correlations between numeric columns, excluding non-numeric columns\n",
        "correlations = historical_df.select_dtypes(include=['number']).corr()\n",
        "\n",
        "# Print the correlation matrix\n",
        "print(correlations)"
      ],
      "metadata": {
        "id": "o5Qyzi0xFXdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'submitted_at' to datetime objects\n",
        "historical_df['submitted_at'] = pd.to_datetime(historical_df['submitted_at'])\n",
        "\n",
        "# Group by time periods (e.g., day, week, month) and calculate performance metrics\n",
        "time_based_performance = historical_df.groupby(pd.Grouper(key='submitted_at', freq='W')).agg(\n",
        "    avg_accuracy=('calculated_accuracy', 'mean'),\n",
        "    total_score=('score', 'mean')\n",
        ").reset_index()\n",
        "\n",
        "# Print the time-based performance\n",
        "print(time_based_performance)"
      ],
      "metadata": {
        "id": "sFyHKB9ZCCho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "mgt1aSYXFn-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'topic_performance' DataFrame from previous analysis\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='quiz', y='avg_accuracy', data=topic_performance_historical)\n",
        "plt.title('Average Accuracy by Topic')\n",
        "plt.xlabel('Topic')\n",
        "plt.ylabel('Average Accuracy')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w309f1GjGLkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'difficulty_performance' DataFrame from previous analysis\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='difficulty_level', y='avg_accuracy', data=difficulty_performance)\n",
        "plt.title('Average Accuracy by Difficulty Level')\n",
        "plt.xlabel('Difficulty Level')\n",
        "plt.ylabel('Average Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ncuh7cevGN0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x='difficulty_score', y='calculated_accuracy', data=historical_df)\n",
        "plt.title('Correlation between Difficulty and Accuracy')\n",
        "plt.xlabel('Difficulty Score')\n",
        "plt.ylabel('Calculated Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m54zWVFrGQ1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'time_based_performance' DataFrame from previous analysis\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='submitted_at', y='avg_accuracy', data=time_based_performance)\n",
        "plt.title('Average Accuracy Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Average Accuracy')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_oyimG_uGTwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8iajF64yJdxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'your_user_id'"
      ],
      "metadata": {
        "id": "IHsMMUjlKqyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_data = historical_df[historical_df['id'] == user_id]"
      ],
      "metadata": {
        "id": "OrIZsY6_KrOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_topic_performance = user_data.groupby(user_data['quiz'].apply(lambda x: x['topic'])).agg(\n",
        "    avg_accuracy=('accuracy', lambda x: pd.to_numeric(x.str.rstrip(' %')).mean())\n",
        ").reset_index()\n",
        "\n",
        "weak_topics = user_topic_performance[user_topic_performance['avg_accuracy'] < topic_performance_historical['avg_accuracy'].mean()]\n",
        "print(\"Weak Topics for the User:\")\n",
        "print(weak_topics)"
      ],
      "metadata": {
        "id": "d-RFpi9BKs9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Fetch the submission data\n",
        "submission_url = \"https://api.jsonserve.com/rJvd7g\"\n",
        "submission_data = requests.get(submission_url).json()\n",
        "\n",
        "# Print the structure of the submission data\n",
        "print(\"Current Quiz Submission Data:\")\n",
        "print(submission_data)"
      ],
      "metadata": {
        "id": "4WUX8spSK0G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Extract metadata and response map\n",
        "quiz_metadata = submission_data['quiz']\n",
        "response_map = submission_data['response_map']\n",
        "\n",
        "# Create a DataFrame for response map\n",
        "response_df = pd.DataFrame(list(response_map.items()), columns=['question_id', 'selected_option'])\n",
        "\n",
        "# Add topic and performance metrics to the DataFrame\n",
        "response_df['topic'] = quiz_metadata['topic']\n",
        "response_df['accuracy'] = float(submission_data['accuracy'].strip('%')) / 100\n",
        "response_df['correct_answers'] = submission_data['correct_answers']\n",
        "response_df['incorrect_answers'] = submission_data['incorrect_answers']\n",
        "\n",
        "print(\"Response DataFrame:\")\n",
        "print(response_df.head())"
      ],
      "metadata": {
        "id": "9-dURKrDLXEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Historical topic performance\n",
        "historical_topics = topic_performance_historical[['quiz', 'avg_accuracy', 'total_correct_answers']]\n",
        "historical_topics = historical_topics.rename(columns={'quiz': 'topic'})\n",
        "\n",
        "# Ensure 'is_correct' column exists and is of numeric type in current_quiz_df\n",
        "if 'quiz.questions' in current_quiz_df.columns:\n",
        "    current_quiz_df = current_quiz_df.explode('quiz.questions')  # Explode nested questions\n",
        "    current_quiz_df['is_correct'] = current_quiz_df['quiz.questions'].apply(lambda x: x.get('is_correct', False)).astype(bool)\n",
        "\n",
        "# Group by 'quiz.topic' to get current topic performance\n",
        "current_topic_performance = current_quiz_df.groupby('quiz.topic')['is_correct'].mean().reset_index()\n",
        "current_topic_performance = current_topic_performance.rename(columns={'quiz.topic': 'topic', 'is_correct': 'current_accuracy'})\n",
        "\n",
        "# Merge historical and current topic performance\n",
        "topic_comparison = historical_topics.merge(\n",
        "    current_topic_performance,\n",
        "    on='topic',\n",
        "    how='outer',\n",
        "    suffixes=('_historical', '_current')\n",
        ")\n",
        "\n",
        "# Add trend data\n",
        "topic_comparison['trend'] = topic_comparison['current_accuracy'] - topic_comparison['avg_accuracy']\n",
        "\n",
        "# Identify weak areas (accuracy < 70%)\n",
        "weak_topics = topic_comparison[topic_comparison['current_accuracy'] < 0.7]\n",
        "\n",
        "print(\"Topic Performance Comparison:\")\n",
        "print(topic_comparison)\n",
        "print(\"Weak Topics:\")\n",
        "print(weak_topics)"
      ],
      "metadata": {
        "id": "9xoGnYQ7Qnkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming we have historical_df as a DataFrame containing the historical quiz data\n",
        "\n",
        "user_id = 'YcDFSO4ZukTJnnFMgRNVwZTE4j42'  # Replace with the actual user ID\n",
        "\n",
        "# Filter the historical data for the specific user\n",
        "user_data = historical_df[historical_df['user_id'] == user_id]\n",
        "\n",
        "# Inspect the filtered data\n",
        "print(\"User Data:\")\n",
        "print(user_data[['submitted_at', 'accuracy', 'speed', 'initial_mistake_count', 'mistakes_corrected']])"
      ],
      "metadata": {
        "id": "k3IcIhK3RtTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert 'submitted_at' to datetime for better plotting\n",
        "user_data['submitted_at'] = pd.to_datetime(user_data['submitted_at'])\n",
        "\n",
        "# Plot the accuracy over time\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(user_data['submitted_at'], user_data['accuracy'].str.rstrip('%').astype(float), marker='o', label='Accuracy')\n",
        "plt.title(\"Accuracy Trend for User\")\n",
        "plt.xlabel(\"Quiz Date\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pA7iFvkjRvdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the speed (in percentage) over time\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(user_data['submitted_at'], user_data['speed'].astype(float), marker='o', color='green', label='Speed')\n",
        "plt.title(\"Speed Trend for User\")\n",
        "plt.xlabel(\"Quiz Date\")\n",
        "plt.ylabel(\"Speed (%)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bbD87oU2TZ4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot initial mistakes and mistakes corrected over time\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(user_data['submitted_at'], user_data['initial_mistake_count'], marker='o', label='Initial Mistakes')\n",
        "plt.plot(user_data['submitted_at'], user_data['mistakes_corrected'], marker='o', label='Mistakes Corrected')\n",
        "\n",
        "plt.title(\"Mistakes Over Time for User\")\n",
        "plt.xlabel(\"Quiz Date\")\n",
        "plt.ylabel(\"Mistakes Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t1cWvs6NTfS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate percentage change in accuracy, speed, and mistakes corrected\n",
        "user_data['accuracy_change'] = user_data['accuracy'].str.rstrip('%').astype(float).pct_change() * 100\n",
        "user_data['speed_change'] = user_data['speed'].astype(float).pct_change() * 100\n",
        "user_data['mistakes_corrected_change'] = user_data['mistakes_corrected'].pct_change() * 100\n",
        "\n",
        "print(\"Improvement Metrics (Percentage Change):\")\n",
        "print(user_data[['submitted_at', 'accuracy_change', 'speed_change', 'mistakes_corrected_change']])"
      ],
      "metadata": {
        "id": "oxcGwX2aTi7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "identifying weak areas"
      ],
      "metadata": {
        "id": "pE70Eq2ET9K7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define thresholds\n",
        "accuracy_threshold = 70  # Accuracy less than 70%\n",
        "mistakes_threshold = 5   # High initial mistakes\n",
        "\n",
        "# Filter for low accuracy and high mistakes\n",
        "weak_areas = user_data[\n",
        "    (user_data['accuracy'].str.rstrip('%').astype(float) < accuracy_threshold) &\n",
        "    (user_data['initial_mistake_count'] > mistakes_threshold)\n",
        "]\n",
        "\n",
        "print(\"Weak Areas (Low Accuracy & High Mistakes):\")\n",
        "print(weak_areas[['submitted_at', 'accuracy', 'initial_mistake_count', 'quiz']])"
      ],
      "metadata": {
        "id": "80tQlKiWT_QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the topic from the 'quiz' column (assuming it's a dictionary with a 'topic' key)\n",
        "user_data['quiz_topic'] = user_data['quiz'].apply(lambda x: x['topic'])\n",
        "\n",
        "# Group by the extracted topic\n",
        "topic_performance = user_data.groupby('quiz_topic').agg({\n",
        "    'accuracy': lambda x: x.str.rstrip('%').astype(float).mean(),\n",
        "    'initial_mistake_count': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Flag topics where average accuracy is below the threshold (70%)\n",
        "consistent_weak_topics = topic_performance[topic_performance['accuracy'] < accuracy_threshold]\n",
        "\n",
        "print(\"Consistent Weak Topics Across Quizzes (Low Accuracy):\")\n",
        "print(consistent_weak_topics[['quiz_topic', 'accuracy']]) # Changed 'quiz' to 'quiz_topic'"
      ],
      "metadata": {
        "id": "yqGu414OUjTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define a threshold for difficulty-level struggles (accuracy below 70% in hard quizzes)\n",
        "difficulty_accuracy_threshold = 70\n",
        "\n",
        "# Assuming 'initial_mistake_count' and 'better_than' columns exist in user_data\n",
        "\n",
        "# Calculate a difficulty score\n",
        "user_data['difficulty_score'] = (user_data['initial_mistake_count'] / user_data['better_than'])\n",
        "\n",
        "# Define difficulty levels based on score ranges\n",
        "def assign_difficulty(score):\n",
        "    if score <= 0.2:\n",
        "        return 'Easy'\n",
        "    elif score <= 0.5:\n",
        "        return 'Medium'\n",
        "    else:\n",
        "        return 'Hard'\n",
        "\n",
        "user_data['difficulty_level'] = user_data['difficulty_score'].apply(assign_difficulty)\n",
        "\n",
        "# Filter for quizzes where difficulty level is 'hard' and accuracy is low\n",
        "difficulty_struggles = user_data[\n",
        "    (user_data['difficulty_level'] == 'hard') &\n",
        "    (user_data['accuracy'].str.rstrip('%').astype(float) < difficulty_accuracy_threshold)\n",
        "]\n",
        "\n",
        "print(\"Difficulty-Level Struggles (Hard Quizzes with Low Accuracy):\")\n",
        "print(difficulty_struggles[['submitted_at', 'accuracy', 'difficulty_level', 'quiz']])"
      ],
      "metadata": {
        "id": "lQd9AaMAVYNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate percentage change for accuracy, speed, and mistakes\n",
        "user_data['accuracy_change'] = user_data['accuracy'].str.rstrip('%').astype(float).pct_change() * 100\n",
        "user_data['mistakes_corrected_change'] = user_data['mistakes_corrected'].pct_change() * 100\n",
        "user_data['speed_change'] = user_data['speed'].astype(float).pct_change() * 100\n",
        "\n",
        "# Identify negative performance trends (negative change in accuracy, speed, or mistakes corrected)\n",
        "negative_trends = user_data[\n",
        "    (user_data['accuracy_change'] < 0) |\n",
        "    (user_data['mistakes_corrected_change'] < 0) |\n",
        "    (user_data['speed_change'] < 0)\n",
        "]\n",
        "\n",
        "print(\"Negative Performance Trends:\")\n",
        "print(negative_trends[['submitted_at', 'accuracy_change', 'mistakes_corrected_change', 'speed_change', 'quiz']])"
      ],
      "metadata": {
        "id": "_rl72oe5Uzvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine weak areas, difficulty struggles, and negative trends into one list of recommendations\n",
        "recommendations = []\n",
        "\n",
        "# Add recommendations based on weak areas (low accuracy and high mistakes)\n",
        "for _, row in weak_areas.iterrows():\n",
        "    recommendations.append(f\"Focus on improving accuracy in the topic '{row['quiz']['topic']}' where accuracy was {row['accuracy']} and initial mistakes were {row['initial_mistake_count']}.\")  # Accessing nested 'topic'\n",
        "\n",
        "# Add recommendations based on difficulty struggles\n",
        "for _, row in difficulty_struggles.iterrows():\n",
        "    recommendations.append(f\"Practice more questions of '{row['difficulty_level']}' difficulty, especially in the topic '{row['quiz']['topic']}', where accuracy was low.\")\n",
        "\n",
        "# Add recommendations based on consistent weak topics\n",
        "for _, row in consistent_weak_topics.iterrows():\n",
        "    recommendations.append(f\"Revise the topic '{row['quiz_topic']}' as your average accuracy in this topic is {row['accuracy']:.2f}%, which is below the expected threshold.\")\n",
        "\n",
        "# Add recommendations based on negative trends\n",
        "for _, row in negative_trends.iterrows():\n",
        "    recommendation = \"Revisit topics where performance has declined, specifically: \"\n",
        "    if row['accuracy_change'] < 0:\n",
        "        recommendation += f\"accuracy in '{row['quiz']['topic']}' \"\n",
        "    if row['mistakes_corrected_change'] < 0:\n",
        "        recommendation += f\"mistake correction in '{row['quiz']['topic']}' \"\n",
        "    if row['speed_change'] < 0:\n",
        "        recommendation += f\"speed in '{row['quiz']['topic']}' \"\n",
        "    recommendations.append(recommendation)\n",
        "\n",
        "# Remove duplicate recommendations\n",
        "recommendations = list(dict.fromkeys(recommendations))\n",
        "\n",
        "# Print all recommendations\n",
        "print(\"Generated Recommendations:\")\n",
        "for i, recommendation in enumerate(recommendations):\n",
        "    print(f\"{i+1}. {recommendation}\")"
      ],
      "metadata": {
        "id": "wZwFgEGPVDra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VISUALISING ACCURACY TRENDS**"
      ],
      "metadata": {
        "id": "ZkpelowBWWuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Convert 'submitted_at' to datetime if it's not already\n",
        "if not pd.api.types.is_datetime64_any_dtype(user_data['submitted_at']):\n",
        "    user_data['submitted_at'] = pd.to_datetime(user_data['submitted_at'])\n",
        "\n",
        "# Plot accuracy trend over time\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='submitted_at', y='accuracy', data=user_data, marker='o', color='blue')\n",
        "plt.title(\"User's Accuracy Trend Over Time\")\n",
        "plt.xlabel(\"Quiz Date\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.grid(True)\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jNJNC6clVKfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "generating recommendations"
      ],
      "metadata": {
        "id": "PTf8uKL-mNrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_recommendations(user_data, topic_performance_historical, accuracy_threshold=70, mistakes_threshold=5):\n",
        "    \"\"\"\n",
        "    Generates personalized recommendations for the user based on quiz performance.\n",
        "\n",
        "    Args:\n",
        "        user_data (pd.DataFrame): DataFrame containing the user's historical quiz data.\n",
        "        topic_performance_historical (pd.DataFrame): DataFrame containing historical topic performance.\n",
        "        accuracy_threshold (int, optional): Accuracy threshold below which a topic is considered weak. Defaults to 70.\n",
        "        mistakes_threshold (int, optional): Threshold for high initial mistakes. Defaults to 5.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of personalized recommendations for the user.\n",
        "    \"\"\"\n",
        "\n",
        "    recommendations = []\n",
        "\n",
        "    # 1. Weak Areas (Low Accuracy & High Mistakes)\n",
        "    weak_areas = user_data[\n",
        "        (user_data['accuracy'].str.rstrip('%').astype(float) < accuracy_threshold) &\n",
        "        (user_data['initial_mistake_count'] > mistakes_threshold)\n",
        "    ]\n",
        "    for _, row in weak_areas.iterrows():\n",
        "        recommendations.append(f\"Focus on improving accuracy in the topic '{row['quiz']['topic']}' where accuracy was {row['accuracy']} and initial mistakes were {row['initial_mistake_count']}.\")\n",
        "\n",
        "    # 2. Consistent Weak Topics (Low Accuracy Across Quizzes)\n",
        "    user_data['quiz_topic'] = user_data['quiz'].apply(lambda x: x['topic'])\n",
        "    topic_performance = user_data.groupby('quiz_topic').agg({\n",
        "        'accuracy': lambda x: x.str.rstrip('%').astype(float).mean(),\n",
        "        'initial_mistake_count': 'mean'\n",
        "    }).reset_index()\n",
        "    consistent_weak_topics = topic_performance[topic_performance['accuracy'] < accuracy_threshold]\n",
        "    for _, row in consistent_weak_topics.iterrows():\n",
        "        recommendations.append(f\"Revise the topic '{row['quiz_topic']}' as your average accuracy in this topic is {row['accuracy']:.2f}%, which is below the expected threshold.\")\n",
        "\n",
        "    # 3. Difficulty-Level Struggles (Hard Quizzes with Low Accuracy)\n",
        "    user_data['difficulty_score'] = (user_data['initial_mistake_count'] / user_data['better_than'])\n",
        "    user_data['difficulty_level'] = user_data['difficulty_score'].apply(lambda score: 'Easy' if score <= 0.2 else 'Medium' if score <= 0.5 else 'Hard')\n",
        "    difficulty_struggles = user_data[\n",
        "        (user_data['difficulty_level'] == 'Hard') &\n",
        "        (user_data['accuracy'].str.rstrip('%').astype(float) < accuracy_threshold)\n",
        "    ]\n",
        "    for _, row in difficulty_struggles.iterrows():\n",
        "        recommendations.append(f\"Practice more questions of '{row['difficulty_level']}' difficulty, especially in the topic '{row['quiz']['topic']}', where accuracy was low.\")\n",
        "\n",
        "    # 4. Negative Performance Trends (Declining Accuracy, Speed, or Mistakes Corrected)\n",
        "    user_data['accuracy_change'] = user_data['accuracy'].str.rstrip('%').astype(float).pct_change() * 100\n",
        "    user_data['mistakes_corrected_change'] = user_data['mistakes_corrected'].pct_change() * 100\n",
        "    user_data['speed_change'] = user_data['speed'].astype(float).pct_change() * 100\n",
        "    negative_trends = user_data[\n",
        "        (user_data['accuracy_change'] < 0) |\n",
        "        (user_data['mistakes_corrected_change'] < 0) |\n",
        "        (user_data['speed_change'] < 0)\n",
        "    ]\n",
        "    for _, row in negative_trends.iterrows():\n",
        "        recommendation = \"Revisit topics where performance has declined, specifically: \"\n",
        "        if row['accuracy_change'] < 0:\n",
        "            recommendation += f\"accuracy in '{row['quiz']['topic']}' \"\n",
        "        if row['mistakes_corrected_change'] < 0:\n",
        "            recommendation += f\"mistake correction in '{row['quiz']['topic']}' \"\n",
        "        if row['speed_change'] < 0:\n",
        "            recommendation += f\"speed in '{row['quiz']['topic']}' \"\n",
        "        recommendations.append(recommendation)\n",
        "\n",
        "    # Remove duplicate recommendations\n",
        "    recommendations = list(dict.fromkeys(recommendations))\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "# Example usage:\n",
        "recommendations = generate_recommendations(user_data, topic_performance_historical)\n",
        "print(\"Generated Recommendations:\")\n",
        "for i, recommendation in enumerate(recommendations):\n",
        "    print(f\"{i + 1}. {recommendation}\")"
      ],
      "metadata": {
        "id": "85Fi-XVbWeGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Fetch the data\n",
        "historical_quiz_data_url = \"https://api.jsonserve.com/XgAgFJ\"\n",
        "submission_data_url = \"https://api.jsonserve.com/rJvd7g\"\n",
        "current_quiz_data_url = \"https://www.jsonkeeper.com/b/LLQT\"\n",
        "\n",
        "historical_quiz_data = requests.get(historical_quiz_data_url).json()\n",
        "submission_data = requests.get(submission_data_url).json()\n",
        "current_quiz_data = requests.get(current_quiz_data_url).json()\n",
        "\n",
        "# Convert historical quiz data to DataFrame\n",
        "historical_df = pd.DataFrame(historical_quiz_data)\n",
        "\n",
        "# Get all unique user IDs\n",
        "all_user_ids = historical_df['user_id'].unique()\n",
        "\n",
        "# Function to identify weak areas (from previous code)\n",
        "def identify_weak_areas(user_data, accuracy_threshold=70, mistakes_threshold=5):\n",
        "    weak_areas = user_data[\n",
        "        (user_data['accuracy'].str.rstrip('%').astype(float) < accuracy_threshold) &\n",
        "        (user_data['initial_mistake_count'] > mistakes_threshold)\n",
        "    ]\n",
        "    return weak_areas\n",
        "\n",
        "# Function to identify consistent weak topics (from previous code)\n",
        "def identify_consistent_weak_topics(user_data, accuracy_threshold=70):\n",
        "    user_data['quiz_topic'] = user_data['quiz'].apply(lambda x: x.get('topic', 'Unknown')) # Handle missing 'topic'\n",
        "    topic_performance = user_data.groupby('quiz_topic').agg({\n",
        "        'accuracy': lambda x: x.str.rstrip('%').astype(float).mean(),\n",
        "        'initial_mistake_count': 'mean'\n",
        "    }).reset_index()\n",
        "    consistent_weak_topics = topic_performance[topic_performance['accuracy'] < accuracy_threshold]\n",
        "    return consistent_weak_topics\n",
        "\n",
        "# Function to identify difficulty-level struggles (from previous code)\n",
        "def identify_difficulty_struggles(user_data, accuracy_threshold=70):\n",
        "    user_data['difficulty_score'] = (user_data['initial_mistake_count'] / user_data['better_than'])\n",
        "    user_data['difficulty_level'] = user_data['difficulty_score'].apply(lambda score: 'Easy' if score <= 0.2 else 'Medium' if score <= 0.5 else 'Hard')\n",
        "    difficulty_struggles = user_data[\n",
        "        (user_data['difficulty_level'] == 'Hard') &\n",
        "        (user_data['accuracy'].str.rstrip('%').astype(float) < accuracy_threshold)\n",
        "    ]\n",
        "    return difficulty_struggles\n",
        "\n",
        "# Function to identify negative performance trends (from previous code)\n",
        "def identify_negative_trends(user_data):\n",
        "    user_data['accuracy_change'] = user_data['accuracy'].str.rstrip('%').astype(float).pct_change() * 100\n",
        "    user_data['mistakes_corrected_change'] = user_data['mistakes_corrected'].pct_change() * 100\n",
        "    user_data['speed_change'] = user_data['speed'].astype(float).pct_change() * 100\n",
        "    # Replace infinite values with NaN for easier filtering\n",
        "    user_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    negative_trends = user_data[\n",
        "        (user_data['accuracy_change'].fillna(0) < 0) |  # Handle NaN with fillna(0)\n",
        "        (user_data['mistakes_corrected_change'].fillna(0) < 0) |\n",
        "        (user_data['speed_change'].fillna(0) < 0)\n",
        "    ]\n",
        "    return negative_trends\n",
        "\n",
        "\n",
        "# Function to generate user persona (updated)\n",
        "def generate_user_persona(user_id, historical_df, submission_data, current_quiz_data):\n",
        "    \"\"\"Generates a user persona based on the provided data.\"\"\"\n",
        "    # Filter data for the specific user\n",
        "    user_data = historical_df[historical_df['user_id'] == user_id]\n",
        "\n",
        "    # Calculate average accuracy\n",
        "    avg_accuracy = user_data['accuracy'].str.rstrip('%').astype(float).mean()\n",
        "\n",
        "    # Identify preferred topics (example)\n",
        "    preferred_topics = user_data['quiz'].apply(lambda x: x.get('topic', 'Unknown')).value_counts().index.tolist() # Handle missing 'topic'\n",
        "\n",
        "    # Basic learning style (example)\n",
        "    avg_speed = user_data['speed'].astype(float).mean()\n",
        "    learning_style = \"Fast\" if avg_speed > user_data['speed'].astype(float).quantile(0.75) else \"Moderate\"\n",
        "\n",
        "    # Analyze strengths and weaknesses\n",
        "    strengths = []\n",
        "    weaknesses = []\n",
        "\n",
        "    if avg_accuracy > 80:\n",
        "        strengths.append(\"High Overall Accuracy\")\n",
        "    if avg_speed > user_data['speed'].astype(float).quantile(0.75):\n",
        "        strengths.append(\"Fast Quiz Completion\")\n",
        "    # ... (Add more strength analysis)\n",
        "\n",
        "    # Call weakness identification functions\n",
        "    weak_areas = identify_weak_areas(user_data)\n",
        "    consistent_weak_topics = identify_consistent_weak_topics(user_data)\n",
        "    difficulty_struggles = identify_difficulty_struggles(user_data)\n",
        "    negative_trends = identify_negative_trends(user_data)\n",
        "\n",
        "    # Combine weaknesses into a list\n",
        "    weaknesses = []  # Initialize an empty list\n",
        "    weaknesses.extend([\n",
        "        f\"Struggles with accuracy in '{row['quiz']['topic']}' (accuracy: {row['accuracy']}, initial mistakes: {row['initial_mistake_count']})\"\n",
        "        for _, row in weak_areas.iterrows()\n",
        "    ])\n",
        "    weaknesses.extend([\n",
        "        f\"Consistently underperforms in '{row['quiz_topic']}' (average accuracy: {row['accuracy']:.2f}%)\"\n",
        "        for _, row in consistent_weak_topics.iterrows()\n",
        "    ])\n",
        "    weaknesses.extend([\n",
        "        f\"Faces challenges with 'Hard' difficulty questions, especially in '{row['quiz']['topic']}'\"\n",
        "        for _, row in difficulty_struggles.iterrows()\n",
        "    ])\n",
        "    weaknesses.extend([\n",
        "        \"Shows declining performance in: \" +\n",
        "        \", \".join([f\"{metric} in '{row['quiz']['topic']}'\"\n",
        "                   for metric in ['accuracy', 'mistakes corrected', 'speed']\n",
        "                   if row[f\"{metric.replace(' ', '_')}_change\"] < 0])\n",
        "        for _, row in negative_trends.iterrows()\n",
        "    ])\n",
        "\n",
        "    persona = {\n",
        "      'user_id': user_id,\n",
        "      'average_accuracy': avg_accuracy,\n",
        "      'preferred_topics': preferred_topics,\n",
        "      'learning_style': learning_style,\n",
        "      'strengths': strengths,\n",
        "      'weaknesses': weaknesses  # Use the list of weaknesses\n",
        "    }\n",
        "\n",
        "    return persona\n",
        "\n",
        "# Generate personas for all users\n",
        "user_personas = {}\n",
        "for user_id in all_user_ids:\n",
        "    user_personas[user_id] = generate_user_persona(user_id, historical_df, submission_data, current_quiz_data)\n",
        "\n",
        "# Print the personas\n",
        "for user_id, persona in user_personas.items():\n",
        "    print(f\"User Persona for {user_id}:\")\n",
        "    print(persona)\n",
        "    print(\"-\" * 20)  # Separator"
      ],
      "metadata": {
        "id": "W4k9y3bwlmPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cyFxugLumSSW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}